---
redirect_from:
  - "/ch/11/gradient-descent"
interact_link: content/ch/11/gradient_descent.ipynb
kernel_name: 
kernel_path: content/ch/11
has_widgets: false
title: |-
  Gradient Descent
pagenum: 47
prev_page:
  url: /ch/10/modeling_abs_huber.html
next_page:
  url: /ch/11/gradient_basics.html
suffix: .ipynb
search: loss our model function mean functions gradient descent dataset example tip percentage error found expressions minimize models useful numerical optimization order estimation prediction need precisely define select assumed single does not vary table decided squared minimized also simple mse absolute median however become complicated longer able algebraic huber properties difficult differentiate hand computer address issue using computational method minimizing

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Gradient Descent</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Gradient-Descent-and-Numerical-Optimization">Gradient Descent and Numerical Optimization<a class="anchor-link" href="#Gradient-Descent-and-Numerical-Optimization"> </a></h1><p>In order to use a dataset for estimation and prediction, we need to precisely
define our model and select a loss function. For example, in the tip percentage
dataset, our model assumed that there was a single tip percentage that does not
vary by table. Then, we decided to use the mean squared error loss function and
found the model that minimized the loss function.</p>
<p>We also found that there are simple expressions that minimize the MSE and the
mean absolute error loss functions: the mean and the median. However, as our models
and loss functions become more complicated we will no longer be able to find
useful algebraic expressions for the models that minimize the loss. For
example, the Huber loss has useful properties but is difficult to differentiate
by hand.</p>
<p>We can use the computer to address this issue using gradient descent, a
computational method of minimizing loss functions.</p>

</div>
</div>
</div>
</div>

 


    </main>
    